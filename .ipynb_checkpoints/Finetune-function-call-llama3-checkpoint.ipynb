{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220bf1dd-3a87-4005-af59-63cb4261a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2250816/2159472044.py:7: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import unsloth_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a78b6c7-dd1f-4960-a443-d59aff33db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc4fb78-edc7-44bc-9972-7028ba905ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'query', 'answers', 'tools'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'query', 'answers', 'tools'],\n",
       "     num_rows: 150\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/xlam-function-calling-60k-train_data.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(\"./data/xlam-function-calling-60k-validation_data.pkl\", \"rb\") as f:\n",
    "    validation_data = pickle.load(f)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "validation_dataset = Dataset.from_list(validation_data)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.select(range(2000, 2500))\n",
    "valid_dataset = validation_dataset.select(range(100, 250))\n",
    "train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c9bd89-1f92-48e1-aa09-e7c9859c026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 29110,\n",
       " 'query': \"Fetch the joke of the day from the 'nerdy' category.\",\n",
       " 'answers': '[{\"name\": \"get_joke_of_the_day_by_category\", \"arguments\": {\"category\": \"nerdy\"}}]',\n",
       " 'tools': '[{\"name\": \"get_joke_of_the_day_by_category\", \"description\": \"Fetches the joke of the day from a specified category using the World of Jokes API.\", \"parameters\": {\"category\": {\"description\": \"The category of joke to be fetched.\", \"type\": \"str\", \"default\": \"Money\"}}}]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cda15d-9ed5-49e0-b359-facc4177f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A30. Num GPUs = 1. Max memory: 23.498 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/home/ozoneai/models/text/Llama-3.2-3B-Instruct/\"\n",
    "\n",
    "max_seq_length = 2048     # Unsloth auto supports RoPE Scaling internally!\n",
    "dtype = None              # None for auto detection\n",
    "load_in_4bit = False    # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,  \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a29d71c-c85b-4ee0-89ac-8c1880d2b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True,        # Maps <|im_end|> to <|eot_id|> instead\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ce6c6b-03c9-4f0a-b980-de700853db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None   # No LoftQ, for standard fine-tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5abdde-9f32-47b9-9c6f-1112afd938fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    \n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for query, tools, answers in zip(examples['query'], examples['tools'], examples['answers']):\n",
    "        tool_user = {\n",
    "            \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{tools}\",\n",
    "            \"role\": \"system\"\n",
    "        }\n",
    "        ques_user = {\n",
    "            \"content\": f\"{query}\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "        assistant = {\n",
    "            \"content\": f\"{answers}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        convos.append([tool_user, ques_user, assistant])\n",
    "\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2722eee3-93b9-4aa3-8b9a-ea787f4b5f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324c828482854ea8b46ec6a35595f954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1872aa81354d9fb4de1b4580d71f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'query', 'answers', 'tools', 'text'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'query', 'answers', 'tools', 'text'],\n",
       "     num_rows: 150\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the formatting on dataset\n",
    "dataset_train = train_dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset_validation = valid_dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset_train, dataset_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106fa81e-0bd8-403a-8ea0-007b7596cddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_joke_of_the_day_by_category',\n",
       "  'description': 'Fetches the joke of the day from a specified category using the World of Jokes API.',\n",
       "  'parameters': {'category': {'description': 'The category of joke to be fetched.',\n",
       "    'type': 'str',\n",
       "    'default': 'Money'}}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(train_dataset[0][\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b119c8c-5990-47a4-afd1-0143233e5bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\n",
      "[{\"name\": \"get_joke_of_the_day_by_category\", \"description\": \"Fetches the joke of the day from a specified category using the World of Jokes API.\", \"parameters\": {\"category\": {\"description\": \"The category of joke to be fetched.\", \"type\": \"str\", \"default\": \"Money\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Fetch the joke of the day from the 'nerdy' category.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[{\"name\": \"get_joke_of_the_day_by_category\", \"arguments\": {\"category\": \"nerdy\"}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "350cde30-ab55-402b-bf7d-c502d23e8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=16, # Accumulates gradients to simulate a larger batch\n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-4,             # Sets the learning rate for optimization\n",
    "        num_train_epochs = 3,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        load_best_model_at_end = True,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "        lr_scheduler_type = \"linear\",  #cosine   # Chooses a linear learning rate decay\n",
    "        output_dir = \"../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1\",        \n",
    "        report_to = \"tensorboard\",              # Enables Weights & Biases (W&B) logging\n",
    "        logging_dir=\"../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1/logs\",\n",
    "        logging_steps = 10,                # Sets frequency of logging to W&B\n",
    "        logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "        save_strategy=\"steps\",\n",
    "        eval_strategy=\"steps\",\n",
    "        save_total_limit= 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d5ab552-742d-466d-8953-0fd608d46714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91da01d3546547a8b5ebf73c2bf95ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55146b4d240847018e598fd434d69b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    eval_dataset = dataset_validation,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing=False,\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58be86ff-978d-44ab-947b-3b247de4c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A30. Max memory = 23.498 GB.\n",
      "6.779 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61d8fbc-5447-4d8e-9eba-d229cffbb456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 3 | Total steps = 93\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 09:11, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.471000</td>\n",
       "      <td>1.158546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.969100</td>\n",
       "      <td>0.797320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.754973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.738200</td>\n",
       "      <td>0.729892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.725800</td>\n",
       "      <td>0.707267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.689298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.667442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.647955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c38624a-40a7-4e5e-bc14-85f1c5db63fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1/tokenizer_config.json',\n",
       " '../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1/special_tokens_map.json',\n",
       " '../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1\")\n",
    "tokenizer.save_pretrained(\"../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-function-calling-V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "017d3c06-e828-4956-806b-ae0ab690be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 113.76 out of 251.39 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 70.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# merging the LoRA adapters with the base model\n",
    "\n",
    "model.save_pretrained_merged(\"../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-finetune-func-v1\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5ba38-54ac-40b6-b6db-04feb3875ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
