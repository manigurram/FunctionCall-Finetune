{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f15cd7-a4ab-413c-a4f6-c4c7df1a4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForConditionalGeneration\n",
    "import pickle, json, re, gc\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3202f1f-3fc6-48d1-bd4d-36207f038f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, device(type='cuda'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch_dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe941be-c4cb-4901-acfe-b1abe5f41939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09b1b975f5348609aeeef5c4b0ae667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"../../models/gemma-3-4b-it/\"\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",# eager, flash_attention_2\n",
    "    torch_dtype=torch_dtype, #torch_dtype, auto\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1e1520-cfde-4ad8-afcb-f80996f43bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9530aa0-349c-4bcf-8099-bb340c1e56be",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bd1ad4-bf51-4dcd-ba1a-d938da23fca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'query', 'answers', 'tools'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'query', 'answers', 'tools'],\n",
       "     num_rows: 150\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/xlam-function-calling-60k-train_data.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(\"../data/xlam-function-calling-60k-validation_data.pkl\", \"rb\") as f:\n",
    "    validation_data = pickle.load(f)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "validation_dataset = Dataset.from_list(validation_data)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.select(range(2000, 2500))\n",
    "valid_dataset = validation_dataset.select(range(100, 250))\n",
    "train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f477100-e652-4611-922e-ca6f6255d550",
   "metadata": {},
   "source": [
    "### updated prompt(05-06-2025) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573e9d7a-14df-4a47-86b4-da9299a289a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_structured(sample):\n",
    "    try:\n",
    "        tools = json.loads(sample[\"tools\"])\n",
    "        answers = json.loads(sample[\"answers\"])\n",
    "        user_query = sample[\"query\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error decoding JSON:\", sample)\n",
    "        raise e\n",
    "    \n",
    "    system_prompt = \"You are a helpful assistant that can call functions to help answer user queries. When you need to use a tool, format your response with <function_call> tags containing valid JSON.\"\n",
    "    \n",
    "    tools_formatted = []\n",
    "    for tool in tools:\n",
    "        tool_info = {\n",
    "            \"name\": tool[\"name\"],\n",
    "            \"description\": tool[\"description\"],\n",
    "            \"parameters\": tool.get(\"parameters\", {})\n",
    "        }\n",
    "        tools_formatted.append(json.dumps(tool_info, indent=2))\n",
    "    \n",
    "    tools_text = \"Available tools:\\n\" + \"\\n\\n\".join(tools_formatted)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{tools_text}\\n\\nUser query: {user_query}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": \"\\n\".join(\n",
    "                f\"<function_call>\\n{json.dumps(answer, ensure_ascii=False, separators=(',', ':'))}\\n</function_call>\"  # FIXED: added ensure_ascii=False\n",
    "                for answer in answers\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a90c54e-9dc6-4dbd-b0f1-53670b91f46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8e5735a16a40db98db4af8cc1eef2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638afeb592a544adaacaf69f8a630a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 150\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = train_dataset.map(preprocess_structured, remove_columns=[\"id\", \"query\", \"answers\", \"tools\"])\n",
    "\n",
    "dataset_validation = valid_dataset.map(preprocess_structured, remove_columns=[\"id\", \"query\", \"answers\", \"tools\"])\n",
    "dataset_train, dataset_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a740ffbf-f508-4000-accb-31bf67f5fc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant that can call functions to help answer user queries. When you need to use a tool, format your response with <function_call> tags containing valid JSON.\n",
      "\n",
      "Available tools:\n",
      "{\n",
      "  \"name\": \"get_joke_of_the_day_by_category\",\n",
      "  \"description\": \"Fetches the joke of the day from a specified category using the World of Jokes API.\",\n",
      "  \"parameters\": {\n",
      "    \"category\": {\n",
      "      \"description\": \"The category of joke to be fetched.\",\n",
      "      \"type\": \"str\",\n",
      "      \"default\": \"Money\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "User query: Fetch the joke of the day from the 'nerdy' category.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<function_call>\n",
      "{\"name\":\"get_joke_of_the_day_by_category\",\"arguments\":{\"category\":\"nerdy\"}}\n",
      "</function_call><end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ee6a21-a4db-4ceb-9433-0f2939f002fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32, #16\n",
    "    lora_dropout=0.1, # 0.05\n",
    "    r=32, #16\n",
    "    bias=\"none\",\n",
    "    # target_modules=\"all-linear\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # More specific targeting\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens Not needed without new tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c49a96-1607-4424-bd63-ddac087ff744",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=10, # 10 it will print the loss \n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\", #constant\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=True,\n",
    "    save_total_limit = 2, # number of check points to store , in that 2 check points are saved , last and best \n",
    "    save_steps = 100, # for saving the checkpoints per steps\n",
    "    load_best_model_at_end = True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    torch_compile=False,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3,\n",
    "    gradient_checkpointing=True, # True\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    "    logging_dir=\"/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1/logs\",\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "272f8475-5b72-43c0-aac9-a64c52bafa89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53144e6a-84ae-4753-a6f4-03142344ad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15f3c044d4e48e8a5ace039c86ebff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3521ebf54d4fe89359c26bfe86c970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e6dc67572c4b73ba36cabf6f4b9c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bea0d892b0b4ceaa01089ab4dace945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbeb88c41874fe89821dd2c43f593cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a26907bc0d741e882e18953ff108ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da3fe55c0be453dae3fa28805343b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090462b04b3140128175165216851f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class = tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba23929-e163-4417-897b-f122ed91bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 11:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>21.925600</td>\n",
       "      <td>0.654367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.915700</td>\n",
       "      <td>0.548147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.293900</td>\n",
       "      <td>0.504273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.775700</td>\n",
       "      <td>0.485462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.065400</td>\n",
       "      <td>0.472899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.530200</td>\n",
       "      <td>0.463355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>6.240700</td>\n",
       "      <td>0.459858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.911200</td>\n",
       "      <td>0.459946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.643600</td>\n",
       "      <td>0.459090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=8.610176024898406, metrics={'train_runtime': 726.0552, 'train_samples_per_second': 2.066, 'train_steps_per_second': 0.128, 'total_flos': 1.794163375706256e+16, 'train_loss': 8.610176024898406})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d51d8b6-6b62-4cfc-9d72-c42d10f6d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8cfb6-70bb-4de3-b4d4-18aeecbd12ee",
   "metadata": {},
   "source": [
    "### Saving...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54988175-1c71-4879-ad48-c04a7697f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, Gemma3ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4e6b91-f9e7-4035-916c-673c9595ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, device(type='cuda'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch_dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513b9aa5-328d-48a0-a39a-f1e75f6d2f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d109e5c289df45f89aa18ef01ac7b208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model_id = \"/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1/\"\n",
    "model_name = \"../../models/gemma-3-4b-it\"\n",
    "\n",
    "save_folder = \"/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(model_name,\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       torch_dtype=torch_dtype, # torch_dtype, auto\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5e0335-ab18-4ac8-98b8-49ddaa22d465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e36eb8-d030-41e7-8ba2-210da01a7dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged/tokenizer_config.json',\n",
       " '/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged/special_tokens_map.json',\n",
       " '/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged/tokenizer.model',\n",
       " '/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged/added_tokens.json',\n",
       " '/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.save_pretrained(save_folder)\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ec209f-a8fc-4151-8ea7-603f7e70f2f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3Config {\n",
       "  \"architectures\": [\n",
       "    \"Gemma3ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"boi_token_index\": 255999,\n",
       "  \"eoi_token_index\": 256000,\n",
       "  \"eos_token_id\": [\n",
       "    1,\n",
       "    106\n",
       "  ],\n",
       "  \"image_token_index\": 262144,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"mm_tokens_per_image\": 256,\n",
       "  \"model_type\": \"gemma3\",\n",
       "  \"text_config\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"attn_logit_softcapping\": null,\n",
       "    \"cache_implementation\": \"hybrid\",\n",
       "    \"final_logit_softcapping\": null,\n",
       "    \"head_dim\": 256,\n",
       "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "    \"hidden_size\": 2560,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 10240,\n",
       "    \"max_position_embeddings\": 131072,\n",
       "    \"model_type\": \"gemma3_text\",\n",
       "    \"num_attention_heads\": 8,\n",
       "    \"num_hidden_layers\": 34,\n",
       "    \"num_key_value_heads\": 4,\n",
       "    \"query_pre_attn_scalar\": 256,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_local_base_freq\": 10000.0,\n",
       "    \"rope_scaling\": {\n",
       "      \"factor\": 8.0,\n",
       "      \"rope_type\": \"linear\"\n",
       "    },\n",
       "    \"rope_theta\": 1000000.0,\n",
       "    \"sliding_window\": 1024,\n",
       "    \"sliding_window_pattern\": 6,\n",
       "    \"torch_dtype\": \"bfloat16\",\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 262208\n",
       "  },\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"vision_config\": {\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "    \"hidden_size\": 1152,\n",
       "    \"image_size\": 896,\n",
       "    \"intermediate_size\": 4304,\n",
       "    \"layer_norm_eps\": 1e-06,\n",
       "    \"model_type\": \"siglip_vision_model\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_channels\": 3,\n",
       "    \"num_hidden_layers\": 27,\n",
       "    \"patch_size\": 14,\n",
       "    \"torch_dtype\": \"bfloat16\",\n",
       "    \"vision_use_head\": false\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42439dac-a98e-4c37-983b-9390352e504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.config.to_json_file(f\"{save_folder}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae399cb7-7e28-4981-b325-3eec9f1bee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='../../models/gemma-3-4b-it/', revision=None, inference_mode=True, r=32, target_modules={'gate_proj', 'k_proj', 'up_proj', 'v_proj', 'q_proj', 'o_proj', 'down_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbde5b8-bad2-480c-bf3c-3cffaba03cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/mnt/data1/mani/finetuned_models/gemma-3-4b-it-function-calling-V1-merged/processor_config.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "processor.save_pretrained(save_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
