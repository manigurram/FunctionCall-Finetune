{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2910fe2a-2efd-4f67-ba3b-19216d749690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import pickle\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c348df0-9035-4e16-a0d2-4d17402fc5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A30. Num GPUs = 1. Max memory: 23.498 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc9b92ed2814ec492c5033bf5c37915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 2048     \n",
    "dtype = None\n",
    "load_in_4bit = False  \n",
    "model_name = \"../../mani/models/finetuned_models/Llama-3.2-3B-Instruct-finetune-func-v1\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,        # Trained model either locally or from huggingface\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "decb1a09-1e4c-444e-b9df-5da1a1d46087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_prompts_from_examples(examples):\n",
    "    prompts = []\n",
    "\n",
    "    for query, tools in zip(examples[\"query\"], examples[\"tools\"]):\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant with access to the following tools or function calls. \"\n",
    "                \"Your task is to produce a sequence of tools or function calls necessary to generate a response to the user utterance. \"\n",
    "                \"Use the following tools or function calls as required:\\n\"\n",
    "                f\"{tools}\"\n",
    "            )\n",
    "        }\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "\n",
    "        convo = [system_message, user_message]\n",
    "        prompt = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return {\"text\": prompts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0506c38f-c705-4a53-a4af-25937e5a04d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd3a16ae23d4902bbe5d704cbea3d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'query', 'answers', 'tools', 'text'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/xlam-function-calling-60k-updated-test_data.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "dataset_test = test_dataset.map(create_inference_prompts_from_examples, batched = True,)\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b0d72a3-cd71-41e4-8799-6fc53fe5d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate a response to the user utterance. Use the following tools or function calls as required:\n",
      "[{\"name\": \"get_mempool_v2\", \"description\": \"Retrieve a list of transaction IDs currently in the mempool of the specified blockchain node, representing unconfirmed transactions not yet included in any block.\", \"parameters\": {\"blockchain\": {\"description\": \"The name of the blockchain.\", \"type\": \"str\", \"default\": \"bitcoin\"}, \"pagesize\": {\"description\": \"The number of transactions to return per call. Default and maximum is 1000.\", \"type\": \"int, optional\", \"default\": 1000}, \"page\": {\"description\": \"Specifies the page of returned transactions, starting from 1. If out of range, the closest possible page is returned. Default is 1.\", \"type\": \"int, optional\", \"default\": 1}}}, {\"name\": \"new_ios_apps\", \"description\": \"Fetch a list of new iOS apps from the App Store using the RapidAPI service.\", \"parameters\": {\"country\": {\"description\": \"The country code for the App Store. Defaults to 'us'.\", \"type\": \"str, optional\", \"default\": \"us\"}, \"category\": {\"description\": \"The category code for the type of apps. Defaults to '6016'.\", \"type\": \"str, optional\", \"default\": \"6016\"}, \"lang\": {\"description\": \"The language code for the App Store content. Defaults to 'en'.\", \"type\": \"str, optional\", \"default\": \"en\"}, \"num\": {\"description\": \"The number of results to return. Defaults to 100.\", \"type\": \"int, optional\", \"default\": \"100\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Retrieve a list of transaction IDs currently in the mempool of the Bitcoin blockchain node.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test[\"text\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff21d572-1405-461b-ba51-358a9d416079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate a response to the user utterance. Use the following tools or function calls as required:\n",
      "[{\"name\": \"find_longest_word\", \"description\": \"Finds the longest word in a list of words.\", \"parameters\": {\"words\": {\"description\": \"A list of words.\", \"type\": \"List[str]\"}}}, {\"name\": \"is_valid_parentheses\", \"description\": \"Checks if a string contains valid parentheses.\", \"parameters\": {\"s\": {\"description\": \"The input string.\", \"type\": \"str\"}}}, {\"name\": \"get_range\", \"description\": \"Helper function to format the range string.\", \"parameters\": {\"start\": {\"description\": \"The start of the range.\", \"type\": \"int\"}, \"end\": {\"description\": \"The end of the range.\", \"type\": \"int\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the range string for numbers from 10 to 20 and from 30 to 40?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset_test[270]\n",
    "\n",
    "print(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10027a13-6026-4412-859e-03ac97be63c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"get_range\", \"arguments\": {\"start\": 10, \"end\": 20}}, {\"name\": \"get_range\", \"arguments\": {\"start\": 30, \"end\": 40}}]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(test_data[\"text\"], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d83189e-67de-4813-b615-0f6fa853b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY is : What is the range string for numbers from 10 to 20 and from 30 to 40?\n"
     ]
    }
   ],
   "source": [
    "print(\"QUERY is :\", test_data[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d986e632-9fe8-4e2a-9f90-2dc0d2d0e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Output : [{\"name\": \"get_range\", \"arguments\": {\"start\": 10, \"end\": 20}}, {\"name\": \"get_range\", \"arguments\": {\"start\": 30, \"end\": 40}}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Output :\", test_data[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5a2cb-5ab3-4687-825f-473a48cc7a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0450e18e-cab9-4f92-960b-e85abeb881f3",
   "metadata": {},
   "source": [
    "### With out Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2584f4c-1e6f-4b47-b105-66b210709162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A30. Num GPUs = 1. Max memory: 23.498 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 2048     \n",
    "dtype = None\n",
    "load_in_4bit = False  \n",
    "\n",
    "model_name = \"../models/text/Llama-3.2-3B-Instruct/\"\n",
    "\n",
    "wf_model, wf_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,        # Trained model either locally or from huggingface\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(wf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b8f8eb-e159-48e4-a405-e0151ecbd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_prompts_from_examples(examples):\n",
    "    prompts = []\n",
    "\n",
    "    for query, tools in zip(examples[\"query\"], examples[\"tools\"]):\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant with access to the following tools or function calls. \"\n",
    "                \"Your task is to produce a sequence of tools or function calls necessary to generate a response to the user utterance. \"\n",
    "                \"Use the following tools or function calls as required:\\n\"\n",
    "                f\"{tools}\"\n",
    "            )\n",
    "        }\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "\n",
    "        convo = [system_message, user_message]\n",
    "        prompt = wf_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return {\"text\": prompts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03fd748f-d9dd-4fd4-b058-517396f7a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate a response to the user utterance. Use the following tools or function calls as required:\n",
      "[{\"name\": \"find_longest_word\", \"description\": \"Finds the longest word in a list of words.\", \"parameters\": {\"words\": {\"description\": \"A list of words.\", \"type\": \"List[str]\"}}}, {\"name\": \"is_valid_parentheses\", \"description\": \"Checks if a string contains valid parentheses.\", \"parameters\": {\"s\": {\"description\": \"The input string.\", \"type\": \"str\"}}}, {\"name\": \"get_range\", \"description\": \"Helper function to format the range string.\", \"parameters\": {\"start\": {\"description\": \"The start of the range.\", \"type\": \"int\"}, \"end\": {\"description\": \"The end of the range.\", \"type\": \"int\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the range string for numbers from 10 to 20 and from 30 to 40?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset_test[270]\n",
    "\n",
    "print(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a451affe-328d-4e6a-895c-879e2ab35a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate the range string for numbers from 10 to 20 and from 30 to 40, you can use the following function call:\n",
      "\n",
      "```\n",
      "get_range(start=10, end=20)\n",
      "get_range(start=30, end=40)\n",
      "```\n",
      "\n",
      "This will return the range string for each of the two intervals.\n"
     ]
    }
   ],
   "source": [
    "inputs = wf_tokenizer(test_data[\"text\"], return_tensors=\"pt\", add_special_tokens=False).to(wf_model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = wf_model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = wf_tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608222a4-2cb6-4754-a2e5-36f4ac7cfebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7870e2d-5801-49cd-80b3-9eb6d32f67ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
