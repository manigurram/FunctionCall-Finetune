{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45fa36d4-3c79-4283-9ac1-57d97c094334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForConditionalGeneration\n",
    "import pickle, json, re\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba810cd2-b4d8-4ff8-ae9c-1dcd070ee51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch_dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139c57c4-4f42-44fe-a713-438fe427c4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f3eb5f871f49a7b34f838b2b3fa2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  )\n",
       "  (language_model): Gemma3ForCausalLM(\n",
       "    (model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../finetuned_models/gemma-3-4b-it-function-calling-V1-merged/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\", \n",
    "                                                       torch_dtype=torch_dtype#torch_dtype, auto\n",
    "                                                      )\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype, device_map=\"auto\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f4d04f-a127-4481-8bfe-e1ada6107a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference_preprocessing(batch):\n",
    "    results = {\"id\": [], \"formatted_prompt\": []}\n",
    "    \n",
    "    for tools_json, answers_json, query, sample_id in zip(batch[\"tools\"], batch[\"answers\"], \n",
    "                                                          batch[\"query\"], batch[\"id\"]):\n",
    "        try:\n",
    "            # Parse JSON strings safely\n",
    "            tools = json.loads(tools_json) if isinstance(tools_json, str) else tools_json\n",
    "            expected_answers = json.loads(answers_json) if isinstance(answers_json, str) else answers_json\n",
    "            \n",
    "            # Create system prompt (matches training format)\n",
    "            system_prompt = \"You are a helpful assistant that can call functions to help answer user queries. When you need to use a tool, format your response with <function_call> tags containing valid JSON. Always provide the function call in the exact format requested.\"\n",
    "            \n",
    "            # Format available tools (consistent with training)\n",
    "            tools_formatted = []\n",
    "            for tool in tools:\n",
    "                tool_info = {\n",
    "                    \"name\": tool[\"name\"],\n",
    "                    \"description\": tool[\"description\"],\n",
    "                    \"parameters\": tool.get(\"parameters\", {})\n",
    "                }\n",
    "                tools_formatted.append(json.dumps(tool_info, indent=2))\n",
    "            \n",
    "            tools_text = \"Available tools:\\n\" + \"\\n\\n\".join(tools_formatted)\n",
    "            \n",
    "            # Create messages (matches training structure)\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{tools_text}\\n\\nUser query: {query}\"}\n",
    "            ]\n",
    "            \n",
    "            # Apply tokenizer's chat template for generation\n",
    "            formatted_prompt = tokenizer.apply_chat_template(messages,\n",
    "                                                             add_generation_prompt=True,\n",
    "                                                             tokenize=False,\n",
    "                                                             dtype=torch_dtype)\n",
    "            \n",
    "            results[\"id\"].append(sample_id)\n",
    "            results[\"formatted_prompt\"].append(formatted_prompt)\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            print(f\"Error processing sample {sample_id}: {e}\")\n",
    "            print(f\"Tools: {tools_json}\")\n",
    "            print(f\"Query: {query}\")\n",
    "            # Skip this sample or add empty/default values\n",
    "            continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0933e92-b53e-442f-8815-0d260b13381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3833c35f95430fa97f824696fd9a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'query', 'answers', 'tools', 'formatted_prompt'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/xlam-function-calling-60k-updated-test_data.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "dataset_test = test_dataset.map(batch_inference_preprocessing, batched = True,)\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a72783a3-db41-4fa7-912e-fc3563264fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant that can call functions to help answer user queries. When you need to use a tool, format your response with <function_call> tags containing valid JSON. Always provide the function call in the exact format requested.\n",
      "\n",
      "Available tools:\n",
      "{\n",
      "  \"name\": \"fibonacci\",\n",
      "  \"description\": \"Calculates the nth Fibonacci number.\",\n",
      "  \"parameters\": {\n",
      "    \"n\": {\n",
      "      \"description\": \"The position of the Fibonacci number.\",\n",
      "      \"type\": \"int\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "{\n",
      "  \"name\": \"generate_password\",\n",
      "  \"description\": \"Generates a random password of specified length and character types.\",\n",
      "  \"parameters\": {\n",
      "    \"length\": {\n",
      "      \"description\": \"The length of the password. Defaults to 12.\",\n",
      "      \"type\": \"int, optional\",\n",
      "      \"default\": 12\n",
      "    },\n",
      "    \"include_special\": {\n",
      "      \"description\": \"Whether to include special characters in the password. Defaults to True.\",\n",
      "      \"type\": \"bool, optional\",\n",
      "      \"default\": true\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "{\n",
      "  \"name\": \"is_subset\",\n",
      "  \"description\": \"Checks if one set is a subset of another set.\",\n",
      "  \"parameters\": {\n",
      "    \"set1\": {\n",
      "      \"description\": \"The first set.\",\n",
      "      \"type\": \"set\"\n",
      "    },\n",
      "    \"set2\": {\n",
      "      \"description\": \"The second set.\",\n",
      "      \"type\": \"set\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "User query: Calculate the 12th Fibonacci number and generate a random password of length 10 without special characters<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset_test[370]\n",
    "\n",
    "print(test_data[\"formatted_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c2f623-1cb4-4f93-bec5-ad88eb183312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rnd/workspace/mani/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/rnd/workspace/mani/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function_call>\n",
      "{\"name\":\"fibonacci\",\"arguments\":{\"n\":12}}\n",
      "</function_call>\n",
      "<function_call>\n",
      "{\"name\":\"generate_password\",\"arguments\":{\"length\":10,\"include_special\":false}}\n",
      "</function_call>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(test_data[\"formatted_prompt\"], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274f822b-538a-41d1-821d-c2ae4fa65c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(text):\n",
    "    try:\n",
    "        matches = re.findall(r\"<function_call>\\s*(\\{.*?\\})\\s*</function_call>\", text, re.DOTALL)\n",
    "        tool_calls = []\n",
    "        for match in matches:\n",
    "            # Ensure JSON is stripped of leading/trailing whitespace\n",
    "            cleaned_json = match.strip()\n",
    "            data = json.loads(cleaned_json)\n",
    "            tool_calls.append(data)\n",
    "        return tool_calls\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391c4f53-e361-4e56-922a-c7187a7e20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY is : Calculate the 12th Fibonacci number and generate a random password of length 10 without special characters\n",
      "\n",
      " After Fine-tuning Output: [{'name': 'fibonacci', 'arguments': {'n': 12}}, {'name': 'generate_password', 'arguments': {'length': 10, 'include_special': False}}]\n"
     ]
    }
   ],
   "source": [
    "print(\"QUERY is :\", test_data[\"query\"])\n",
    "print(\"\\n After Fine-tuning Output:\", extract_tool_calls(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed4b651-ef16-4dd7-9143-3cba8ee5d9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Output : [{\"name\": \"fibonacci\", \"arguments\": {\"n\": 12}}, {\"name\": \"generate_password\", \"arguments\": {\"length\": 10, \"include_special\": false}}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Output :\", test_data[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99482e4c-1781-43bf-8cf4-7cc24c0530aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b366779-fb0e-45a2-988b-00b003d27565",
   "metadata": {},
   "source": [
    "### With out finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15616f7-0c32-44ab-af0b-f499c35f8e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"../../models/gemma-3-4b-it/\"\n",
    "\n",
    "wf_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "wf_model = Gemma3ForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\", \n",
    "                                                       torch_dtype=torch_dtype#torch_dtype, auto\n",
    "                                                      )\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype, device_map=\"auto\")\n",
    "wf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c91c32-e4e7-40a4-aad6-6484fddf79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference_preprocessing(batch):\n",
    "    results = {\"id\": [], \"formatted_prompt\": []}\n",
    "    \n",
    "    for tools_json, answers_json, query, sample_id in zip(batch[\"tools\"], batch[\"answers\"], \n",
    "                                                          batch[\"query\"], batch[\"id\"]):\n",
    "        try:\n",
    "            # Parse JSON strings safely\n",
    "            tools = json.loads(tools_json) if isinstance(tools_json, str) else tools_json\n",
    "            expected_answers = json.loads(answers_json) if isinstance(answers_json, str) else answers_json\n",
    "            \n",
    "            # Create system prompt (matches training format)\n",
    "            system_prompt = \"You are a helpful assistant that can call functions to help answer user queries. When you need to use a tool, format your response with <function_call> tags containing valid JSON. Always provide the function call in the exact format requested.\"\n",
    "            \n",
    "            # Format available tools (consistent with training)\n",
    "            tools_formatted = []\n",
    "            for tool in tools:\n",
    "                tool_info = {\n",
    "                    \"name\": tool[\"name\"],\n",
    "                    \"description\": tool[\"description\"],\n",
    "                    \"parameters\": tool.get(\"parameters\", {})\n",
    "                }\n",
    "                tools_formatted.append(json.dumps(tool_info, indent=2))\n",
    "            \n",
    "            tools_text = \"Available tools:\\n\" + \"\\n\\n\".join(tools_formatted)\n",
    "            \n",
    "            # Create messages (matches training structure)\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{tools_text}\\n\\nUser query: {query}\"}\n",
    "            ]\n",
    "            \n",
    "            # Apply tokenizer's chat template for generation\n",
    "            formatted_prompt = wf_tokenizer.apply_chat_template(messages,\n",
    "                                                             add_generation_prompt=True,\n",
    "                                                             tokenize=False,\n",
    "                                                             dtype=torch_dtype)\n",
    "            \n",
    "            results[\"id\"].append(sample_id)\n",
    "            results[\"formatted_prompt\"].append(formatted_prompt)\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "            print(f\"Error processing sample {sample_id}: {e}\")\n",
    "            print(f\"Tools: {tools_json}\")\n",
    "            print(f\"Query: {query}\")\n",
    "            # Skip this sample or add empty/default values\n",
    "            continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8414c1b-6f99-4fc6-b22f-535faa4a1019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3833c35f95430fa97f824696fd9a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'query', 'answers', 'tools', 'formatted_prompt'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/xlam-function-calling-60k-updated-test_data.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "dataset_test = test_dataset.map(batch_inference_preprocessing, batched = True,)\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf2d700-987d-49db-8df3-aa60ba4da563",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset_test[370]\n",
    "\n",
    "# print(test_data[\"formatted_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cc50b29-c0b7-4362-930e-2cceaa75d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s the breakdown of your request and the corresponding function calls:\n",
      "\n",
      "First, I need to calculate the 12th Fibonacci number. I'll use the `fibonacci` tool for this.\n",
      "\n",
      "<function_call>\n",
      "{\n",
      "  \"tool\": \"fibonacci\",\n",
      "  \"parameters\": {\n",
      "    \"n\": 12\n",
      "  }\n",
      "}\n",
      "</function_call>\n",
      "\n",
      "Second, I need to generate a random password of length 10 without\n"
     ]
    }
   ],
   "source": [
    "inputs = wf_tokenizer(test_data[\"formatted_prompt\"], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = wf_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = wf_tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e9f707-9505-4329-8c58-64c104a778e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"fibonacci\", \"arguments\": {\"n\": 12}}, {\"name\": \"generate_password\", \"arguments\": {\"length\": 10, \"include_special\": false}}]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40499d5b-c7fa-4a81-9513-41dc149e714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the 12th Fibonacci number and generate a random password of length 10 without special characters\n"
     ]
    }
   ],
   "source": [
    "print(test_data[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd21cad-e56e-4594-a8f3-51b1c29d7375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
